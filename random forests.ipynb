{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree and Random Forest Classifiers\n",
    "\n",
    "1. Why are tree based methods sometimes preferable?\n",
    "2. How are decision trees constructed?\n",
    "3. How do random forests improve on decision trees?\n",
    "4. Choosing random forest parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Why are tree based methods sometimes preferable?\n",
    "\n",
    "When relationships between predictors and response variables are nonlinear, linear methods like logistic regression are unable to produce good classifications without feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.zeros(100)\n",
    "classes[25:75] = 1\n",
    "predictor = np.arange(1, 101)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(predictor, classes)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,4))\n",
    "\n",
    "ax[0].scatter(predictor, classes)\n",
    "ax[0].set_xlabel('predictor')\n",
    "ax[0].set_ylabel('class')\n",
    "ax[0].set_ylim(-0.1,1.1)\n",
    "\n",
    "ax[1].scatter(predictor, log_reg.predict(predictor))\n",
    "ax[1].set_xlabel('predictor')\n",
    "ax[1].set_ylabel('predicted class')\n",
    "ax[1].set_ylim(-0.1,1.1)\n",
    "\n",
    "fig.suptitle('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a transformation that will give good predictions\n",
    "\n",
    "transformed_predictor = \n",
    "\n",
    "log_reg_transformed = LogisticRegression()\n",
    "log_reg_transformed.fit(transformed_predictor, classes)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,4))\n",
    "\n",
    "ax[0].scatter(transformed_predictor, classes)\n",
    "ax[0].set_xlabel('transformed predictor')\n",
    "ax[0].set_ylabel('class')\n",
    "ax[0].set_ylim(-0.1,1.1)\n",
    "\n",
    "ax[1].scatter(predictor, log_reg_transformed.predict(transformed_predictor))\n",
    "ax[1].set_xlabel('predictor')\n",
    "ax[1].set_ylabel('predicted class')\n",
    "ax[1].set_ylim(-0.1,1.1)\n",
    "\n",
    "fig.suptitle('Logistic Regression with Feature Engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(predictor, classes)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,4))\n",
    "\n",
    "ax[0].scatter(predictor, classes)\n",
    "ax[0].set_xlabel('predictor')\n",
    "ax[0].set_ylabel('class')\n",
    "ax[0].set_ylim(-0.1,1.1)\n",
    "\n",
    "ax[1].scatter(predictor, tree.predict(predictor))\n",
    "ax[1].set_xlabel('predictor')\n",
    "ax[1].set_ylabel('predicted class')\n",
    "ax[1].set_ylim(-0.1,1.1)\n",
    "\n",
    "fig.suptitle('Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How are decision trees constructed?\n",
    "\n",
    "Decision trees work like a flowchart. In the above example, we might have...\n",
    "\n",
    "<img src=\"decision_tree.png\" width=400>\n",
    "\n",
    "The decision points are chosen based on minimizing **Gini Impurity**.\n",
    "\n",
    "**Gini Impurity** is calculated as the sum of squares of the probability an observation belongs to each class.\n",
    "\n",
    "$$ G = \\sum_{c}p_c (1-p_c)$$\n",
    "\n",
    "For example if in a group we have 3 members of class 0 and 2 of class 1, then \n",
    "\n",
    "$$ G = 0.6 \\times 0.4 + 0.4 \\times 0.6 = 2(0.6 \\times 0.4) = 0.48$$\n",
    "\n",
    "This is the Gini impurity for a group... but we want to find the Gini impurity for a split!\n",
    "\n",
    "For a split, we want to use a weighted combination based on the number of samples on each side of the split. So if we have a left side with 5 samples, 3 in class 0 and 2 in class 1, and a right side with 15 samples, all of which are in class 0...\n",
    "\n",
    "$$ G = 2(0.6 \\times 0.4)  \\frac{5}{20} + 2(1.0 \\times 0.0) \\frac{15}{20} = 0.12$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (6,4))\n",
    "\n",
    "ax.scatter(predictor, classes)\n",
    "ax.set_xlabel('predictor')\n",
    "ax.set_ylabel('class')\n",
    "ax.set_ylim(-0.1,1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above data, what would be the Gini impurity if we split on...\n",
    "\n",
    "- predictor > 10\n",
    "- predictor > 25\n",
    "- predictor > 50\n",
    "\n",
    "Which of these is the best split?\n",
    "\n",
    "Is there a difference between splitting on... \n",
    "- predictor > 75 \n",
    "- predictor > 75.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do Random Forests improve on Decision Trees?\n",
    "\n",
    "Decision trees **always** are the same given the same data. \n",
    "1. Find the best split\n",
    "2. For each subnode, find the best split.\n",
    "3. Continue until every leaf has 0 impurity\n",
    "\n",
    "This makes them very good at finding patterns within training data, but can do poorly on out of sample data. Random forests make some improvements on out of sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)\n",
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)\n",
    "print(iris.target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "np.mean(cross_val_score(tree, iris.data, iris.target, cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "cv = KFold(10, shuffle=True, random_state=42)\n",
    "np.mean(cross_val_score(rf, iris.data, iris.target, cv=cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Random Forest** is a set of many decision trees with a few changes.\n",
    "\n",
    "1. At every node we will only consider a subset of the features to look at for determining a best split.\n",
    "2. For every tree we will use a bootstrap sample of our training data instead of the full training data.\n",
    "\n",
    "To make predictions, the trees will 'vote' on a classification. Features which are more influential in splits across all the trees are more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf.fit(iris.data, iris.target)\n",
    "print(\"Feature Importances:\")\n",
    "for k, v in dict(zip(iris.feature_names, rf.feature_importances_)).items():\n",
    "    print(k, \":\", '{:0.3f}'.format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try on some new data!\n",
    "\n",
    "sklearn has a \"make classification\" function to generate data for classification problems.\n",
    "\n",
    "What are the most important features for classification in this sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, target = make_classification(n_samples=1000, random_state=42)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choosing Random Forest Parameters\n",
    "\n",
    "Random forests (and decision trees) have a number of parameters you can tweak. Some are overlapping. \n",
    "\n",
    "We can create weaker learners (which are more generalizable when voting) by reducing the depth of each decision tree. A few parameters affect this directly:\n",
    "\n",
    "- max_leaf_nodes determines the maximum number of terminal nodes\n",
    "- max_depth determines the maximum depth of the tree\n",
    "\n",
    "A few parameters affect the depth indirectly:\n",
    "- min_impurity_decrease will stop building the tree if new splits do not sufficiently decrease impurity\n",
    "- min_impurity_split \n",
    "- min_samples_leaf will stop building the tree if a leaf would have fewer than the minimum samples allowed\n",
    "- min_samples_split will stop building the tree if there are fewer than the minimum samples allowed available for making a split\n",
    "\n",
    "We can also change the number of features considered for a split at each decision point:\n",
    "- max_features\n",
    "\n",
    "And we can change the number of trees in the forest:\n",
    "- n_estimators\n",
    "\n",
    "You can automatically find a best feature through the use of GridSearchCV, but if you try to search over multiple parameters, it could take a long time!\n",
    "\n",
    "For example, if we want to consider 4 options for max_depth, 3 for max_features, and we set n_estimators to 1000, with 10-fold cross validation then we will be fitting $4 \\times 3 \\times 1000 \\times 10 = 120000$ trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "param_grid = {'classifier__max_leaf_nodes': [2,4,8]}\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using a grid search for max_depth in (1,2,4) and max_features in (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
